{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOqAnBqNc+PVWlZwvhv4jG1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Model Selection\n","Often, we won't know beforehand which features to use or engineer for our models, especially when there are many features to choose from. It would be nice if we could determine which features to use before we invest any time in feature engineering.\n","\n","\n","Instead of having just one model, we'll often need to craft **multiple models with different combinations** of features to try to maximize predictive ability.\n","\n"],"metadata":{"id":"XEQEpHI6hD7H"}},{"cell_type":"markdown","source":["# Sequential Feature Selection\n","\n","![image.png](data:image/png;base64,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)"],"metadata":{"id":"qBO72ibohsdP"}},{"cell_type":"markdown","source":["Intuitively, we might want to include both features, **X1 and X2**, in a regression model. What this approach fails to consider is that X1  and X2\n"," might be correlated with each other. If this is the case, having X1 in a model alone will produce similar results to having X2 in a model, but including them both won't add extra benefit. We might mistakenly believe X2 is a useful feature and attempt to use it in future models, making them more complex than they need to be.\n","\n"," This weakness highlights a need for an approach that selects groups of features for a model. This process is called sequential feature selection, also known as subset selection. scikit-learn provides some functionality for implementing subset selection.\n","\n","There are two main methods of **sequential feature selection** that we'll learn: **forward selection and backward selection.** Both methods produce a model with a subset of features that perform well based on some metric, such as MSE, but they differ in their approach to how the features are chosen."],"metadata":{"id":"ZmCA_4zIhzWY"}},{"cell_type":"markdown","source":["###fORWARD SELECTION\n","\n","In forward selection, we start with an intercept-only model (a regression model without predictors). Then, the algorithm iterates over each feature in the dataset to see which one would produce the best model if it were added. The metric used to define \"best\" is a cross-validation score such as MSE or accuracy. After iterating through each feature, the one that produces the best metric is added to the model. Once this feature is added, this process is repeated until we reach some pre-specified number of features or the metric does not improve substantially with the addition of more features.\n","\n","The class that implements forward selection is the SequentialFeatureSelector class.\n","\n","    from sklearn.feature_selection import SequentialFeatureSelector\n","    from sklearn.linear_model import LinearRegression\n","\n","    lm = LinearRegression()\n","\n","    forward_lm = SequentialFeatureSelector(estimator=lm,\n","                                          n_features_to_select=2,\n","                                          direction=\"forward\")\n","    forward_lm.fit(X, y)\n","\n","For now, we'll consider three parameters when dealing with the SequentialFeatureSelector class:\n","\n","`estimator`: this is an object used to construct the models during feature selection, such as LinearRegression or RandomForestClassifier.\n","\n","`n_features_to_select`: this is a positive integer that describes how many features we want to be used in the resulting model.\n","\n","`direction`: this is a string that describes the type of sequential feature selection we want to do (\"forward\" or \"backward\") as a string.\n","\n","In order to see what features were actually included in the model, we can use the `get_feature_names_out()` method after creating a SequentialFeatureSelector() object and calling the `fit()` method on it.\n","\n","    features = forward_lm.get_feature_names_out()"],"metadata":{"id":"3AwoAUOuiXoE"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8HVbWlBWguX_","executionInfo":{"status":"ok","timestamp":1741434335288,"user_tz":180,"elapsed":12926,"user":{"displayName":"Tomaz Filgueira","userId":"15508579070858102815"}},"outputId":"b6fead6b-ccd5-4ca7-cc0d-ffc837bfbe74"},"outputs":[{"output_type":"stream","name":"stdout","text":["['housing_median_age' 'total_rooms' 'total_bedrooms' 'median_income']\n"]}],"source":["# Model Selection\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.feature_selection import SequentialFeatureSelector\n","from sklearn.linear_model import LinearRegression\n","\n","housing = pd.read_csv('housing.csv').dropna()\n","\n","X = housing.drop([\"ocean_proximity\", \"median_house_value\"], axis=1)\n","y = housing[\"median_house_value\"]\n","\n","\n","\n","model = LinearRegression()\n","\n","forward_lm = SequentialFeatureSelector(estimator=model,\n","                                          n_features_to_select=4,\n","                                          direction=\"forward\")\n","forward_lm.fit(X, y)\n","forward_features = forward_lm.get_feature_names_out()\n","print(forward_features)"]},{"cell_type":"markdown","source":["##Backward Selection\n","\n","Forward selection starts with an intercept-only model, while backward selection starts with a full model that uses all the features in the dataset.\n","\n","**Backward selection** then removes a single feature from the model and reevaluates the metric. The algorithm repeats this process for every feature present in the full model. This results in several metrics, each evaluated on a model where a single predictor is removed. Backward selection chooses the model with the best metric and repeats until we reach the desired number of features specified by the `n_features_to_select` argument.\n","\n","Thankfully, backward selection is also implemented by the `SequentialFeatureSelector` class. We only need to change the direction parameter from \"**forward\" to \"backward\"**.\n","\n","    from sklearn.feature_selection import SequentialFeatureSelector\n","    from sklearn.linear_model import LinearRegression\n","\n","    lm = LinearRegression()\n","\n","    backward_lm = SequentialFeatureSelector(estimator=lm,\n","                                            n_features_to_select=2,\n","                                            direction=\"backward\")\n","    backward_lm.fit(X, y)\n","\n","\n","Both of these algorithms make decisions based on the current set of features, and they choose whichever feature best improves the metric immediately. **This does not guarantee **that the subset of features chosen is actually the optimal subset. To find the optimal subset of features, we would need to test all different combinations of predictors.\n","\n","If there are P predictors, then we would need to evaluate $2^p$ models. This can be time-consuming for even a moderate number of predictors, so greedy approaches like forward or backward selection get around this problem by choosing from current subsets."],"metadata":{"id":"9Muf3J9ak4gQ"}},{"cell_type":"code","source":["#Backward selection\n","backward_lm = SequentialFeatureSelector(estimator=model,\n","                                          n_features_to_select=4,\n","                                          direction=\"backward\")\n","\n","backward_lm.fit(X, y)\n","backward_features = backward_lm.get_feature_names_out()\n","n_same_features = forward_features==backward_features\n","print(backward_features)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cu7T_Gs-mGqf","executionInfo":{"status":"ok","timestamp":1741434974528,"user_tz":180,"elapsed":2188,"user":{"displayName":"Tomaz Filgueira","userId":"15508579070858102815"}},"outputId":"0d615ddb-fbf6-46b9-8a9f-4d1e4c56081d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["['longitude' 'latitude' 'total_bedrooms' 'median_income']\n"]}]},{"cell_type":"markdown","source":["# Criterion-Based Selection\n","\n","Forward and backward selection give us algorithms to quickly select a reasonable subset of features for a candidate model.\n","\n","There are some occasions when we might need another tool. One such occasion is when we have limited data, and by extension, don't have much data to allocate to a test set. When this happens, we may want to use a **criterion-based approach** for model selection. We'll compute some criterion (other than test error) for every candidate model that we have and then choose the model that produces the best criterion."],"metadata":{"id":"A3BMna11ndLc"}},{"cell_type":"markdown","source":["## Akaike Information Criterion (AIC)\n","\n","There are several criteria we can use, and the first one we'll look at is the **Akaike Information Criterion (AIC).** The Akaike Information Criterion is a value that estimates the prediction error of a model.\n","\n","$\\text{AIC} = 2p - 2 \\, \\text{ln}(\\hat{L})$\n","\n","Where $p$ represents the number of features using in the model\n","\n","$\\hat{L}$ represents the **likelihood** of the model\n","\n","With linear regression, the likelihood of the model is proportional to sum of squared error (SSE), where $ϵ$ represents the error (i.e., the difference between the actual values in the dataset and predicted values from our model). Without delving into the derivation, the AIC can be written in terms of the SSE as follows:\n","\n","$\\text{AIC} = 2p + n \\, \\text{ln}(\\text{SSE})$\n","\n","where n represents the number of observations in the dataset.\n"],"metadata":{"id":"gKfNfYz-oZb_"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.feature_selection import SequentialFeatureSelector\n","from sklearn.linear_model import LinearRegression\n","\n","from sklearn.metrics import mean_squared_error\n","\n","housing = pd.read_csv('housing.csv').dropna()\n","\n","X = housing.drop([\"ocean_proximity\", \"median_house_value\"], axis=1)\n","y = housing[\"median_house_value\"]\n","\n","model = LinearRegression()\n","\n","def AIC(p, n, SSE):\n","    return 2 * p + 2 * n * np.log(SSE)\n","\n","\n","#sequential with 2 features\n","sfs2 = SequentialFeatureSelector(model,n_features_to_select=2,direction=\"forward\")\n","\n","#sequential with 5 features\n","sfs5 = SequentialFeatureSelector(model,n_features_to_select=5,direction=\"forward\")\n","\n","#fit\n","sfs2.fit(X, y)\n","sfs5.fit(X,y)\n","\n","#get features\n","features2 = sfs2.get_feature_names_out()\n","features5 = sfs5.get_feature_names_out()\n","\n","#create new model with subset\n","X2 = X[features2]\n","X5 = X[features5]\n","n=len(X)\n","\n","lm2 = LinearRegression()\n","lm5 = LinearRegression()\n","\n","#fit new model\n","lm2.fit(X2,y)\n","lm5.fit(X5,y)\n","\n","y_pred2 = lm2.predict(X2)\n","y_pred5 = lm5.predict(X5)\n","\n","#Calculate SSE from mean_squared_error\n","#y - y_pred\n","sse2 = mean_squared_error(y,y_pred2)*n\n","sse5 = mean_squared_error(y,y_pred5)*n\n","\n","aic2 = AIC(len(features2),n,sse2)\n","aic5 = AIC(len(features5),n,sse5)"],"metadata":{"id":"8jMe_K54s3uF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Bayesian Information Criterion\n","\n","The Bayesian Information Criterion (BIC) is just like the AIC, but it uses a different first term:\n","\n","$\\text{BIC} = p \\, \\text{ln}(n) - 2 \\, \\text{ln}(\\hat{L})$\n","\n","In the BIC, the first term — $p \\, \\text{ln}(n)$ — is the product of the number of features (p) and the natural log of the sample size (n). Compared to the AIC, this first term will be larger provided n>=8. Similar to the AIC, the model with the smallest BIC will be considered the best. The BIC can be seen as more critical of larger models, due to $p \\, \\text{ln}(n)$ being much larger than **2p**.\n","\n","As a result, models chosen via BIC tend to have fewer features than those chosen by AIC, but this isn't a concrete rule."],"metadata":{"id":"pc0kfYvhtFPi"}},{"cell_type":"markdown","source":["## Adjusted R²\n","\n","The coefficient of determination, R², is another value that we can calculate for linear regression models. This value estimates how much variation in the outcome the predictors can explain. The coefficient of determination is calculated like this:\n","\n","$\\displaystyle R^2 = 1 - \\frac{\\text{SSE}_p}{\\text{SSE}_\\text{total}}$\n","\n","SSE_p - represents the sum of squared error of the model using p predictors.\n","\n","SSE_total -  represents the total variation in the outcome Y\n","\n","A high R² indicates that the model explains much of the variance in the outcome and is desired. However, we cannot use R² directly as a model selection tool. It turns out that the SSE_p cannot decrease as more predictors are added to the model, so the R² will either remain the same or increase in this case. We should be acutely aware of the danger of overfitting the model, but the R²\n"," doesn't provide a way to do this.\n","\n"," The adjusted R² incorporates a term involving the number of features (p) that serves to penalize complex models. If the model gets larger without offering enough benefit to reducing error, it decreases the adjusted R². So, when we're picking among many models with different numbers of predictors, we can choose the one with the highest adjusted R².\n","\n"," The adjusted R² can, in theory, reach an upper threshold of 1, but this would imply that the model produces perfect predictions, and there would be no need for model selection. However, in practice, no model is a perfect predictor.\n","\n"," $\\displaystyle\\text{Adjusted }R^2 = 1 - \\frac{\\left(1 - R^2\\right) \\, \\left(n - 1\\right)}{n - p - 1}$\n","\n"," When we use **AIC** or **BIC**, we choose models based on the smallest value.\n","\n"," When using the adjusted **R²**, we must choose the model with the largest value. When using these metrics, this is a subtlety that needs to be kept in mind, or you might be led to incorrect conclusions."],"metadata":{"id":"JjtmgKvxt6_f"}},{"cell_type":"markdown","source":["# The Curse of Dimensionality\n","\n","In housing, the number of observations (n) is much higher than the number of predictors (p) in the dataset. In this context, there's no trouble creating models like regressions, but this won't always be the case.\n","\n","In other situations, the reverse might be true: a large number of features (columns) but a smaller number of observations. When this happens, we say that the dataset has **high dimensionality.**\n","\n","In some cases, such as linear regression, high dimensionality prevents us from even creating a model when n < p.\n","\n","A common theme when dealing with high dimensionality is to strategically reduce the number of features we need to consider for the model. This process is called **dimension reduction**. One popular technique for dimension reduction is **principal component analysis (PCA)**. PCA seeks to reduce the dimensions of the data by creating a new dataset with features based off of the original data. This new dataset will have fewer features than observations, which allows us to use all of the techniques we've learned so far.\n"],"metadata":{"id":"pochMNZFO5Xz"}},{"cell_type":"markdown","source":["## PCA\n","\n","A feature created by PCA is called a principal component. A principal component\n","z is a **linear combination of the original features** (X). So, the first principal component z1 would be constructed as:\n","\n","$z_1 = a_1X_1 + ... + a_pX_p$\n","\n","The weights **a1,.....,a_p** are chosen by PCA such that the first principal component contains the most variance in the original dataset. Having high variance means that the values of z1 will be highly spread out. Being more spread out allows us to see how the outcome Y can vary with z1 while preserving the variance in the original dataset.\n","\n","After the first principal component is made, the second is constructed similarly, using all of the variance remaining after the first one is created. The other principal components are created iteratively in this way.\n","\n","Done this way, the first principal component will explain the most variance, and the second wil explain the second-most. When the number of principal components equals the original number of features **p**, then the original dataset is essentially recreated. We can reduce dimensionality by only choosing a small number of principal components that explain some high degree of variance in the original dataset, **say 90%.**"],"metadata":{"id":"lsur54-mPwG2"}},{"cell_type":"markdown","source":["## PCA in SKLearn\n","\n","`scikit-learn` implements PCA under the PCA class in the `decomposition` module. When we instantiate a `PCA()` object, we choose the number of principal components we want to create. Then, we can fit this `PCA()` object on a dataset.\n","\n","    from sklearn.decomposition import PCA\n","    pca = PCA(n_components=3)\n","    pca.fit(X)\n","\n","To understand how much variance is explained by each principal component, we can use the `explained_variance_ratio_` attribute.\n","\n"," To get the total variance explained, we can take the `sum() `of the `explained_variance_ratio_` attribute to get a float value that represents the percentage of the variance being explained."],"metadata":{"id":"D3PiLY0qQyVQ"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.feature_selection import SequentialFeatureSelector\n","from sklearn.linear_model import LinearRegression\n","from sklearn.decomposition import PCA\n","\n","housing = pd.read_csv('housing.csv').dropna()\n","\n","X = housing.drop([\"ocean_proximity\", \"median_house_value\"], axis=1)\n","y = housing[\"median_house_value\"]\n","\n","model = LinearRegression()\n","\n","#instantiate with 1 component\n","pca = PCA(n_components=1)\n","pca.fit(X)\n","\n","var_explained_1 = pca.explained_variance_ratio_[0]\n","\n","#2 components represents 99% of dataset variance\n","n_pc_99= 2"],"metadata":{"id":"P1Sx_KaaRPfF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"zw78t6bETo8w"}}]}