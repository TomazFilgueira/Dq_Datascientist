{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19bd6e9a",
   "metadata": {},
   "source": [
    "# Import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb2cb30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:67: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:75: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:67: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:75: SyntaxWarning: invalid escape sequence '\\('\n",
      "/tmp/ipykernel_2383/3450759038.py:67: SyntaxWarning: invalid escape sequence '\\('\n",
      "  y = re.findall(\"\\(.+\\)\", x)\n",
      "/tmp/ipykernel_2383/3450759038.py:75: SyntaxWarning: invalid escape sequence '\\('\n",
      "  y = re.findall(\"\\(.+\\)\", x)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "data_files = [\n",
    "\"https://raw.githubusercontent.com/TomazFilgueira/Dq_Datascientist/refs/heads/main/3%20-%20Data_Cleaning/03_Data_Cleaning_Walktrough/data/ap_2010.csv\",\n",
    "\"https://raw.githubusercontent.com/TomazFilgueira/Dq_Datascientist/refs/heads/main/3%20-%20Data_Cleaning/03_Data_Cleaning_Walktrough/data/class_size.csv\",\n",
    "\"https://raw.githubusercontent.com/TomazFilgueira/Dq_Datascientist/refs/heads/main/3%20-%20Data_Cleaning/03_Data_Cleaning_Walktrough/data/demographics.csv\",\n",
    "\"https://raw.githubusercontent.com/TomazFilgueira/Dq_Datascientist/refs/heads/main/3%20-%20Data_Cleaning/03_Data_Cleaning_Walktrough/data/graduation.csv\",\n",
    "\"https://raw.githubusercontent.com/TomazFilgueira/Dq_Datascientist/refs/heads/main/3%20-%20Data_Cleaning/03_Data_Cleaning_Walktrough/data/hs_directory.csv\",\n",
    "\"https://raw.githubusercontent.com/TomazFilgueira/Dq_Datascientist/refs/heads/main/3%20-%20Data_Cleaning/03_Data_Cleaning_Walktrough/data/sat_results.csv\",\n",
    "]\n",
    "data = {}\n",
    "\n",
    "for d in data_files:\n",
    "    file = pd.read_csv(d)\n",
    "    #get \"name\" without .csv\n",
    "    name = d.rsplit('data/')[1]\n",
    "    name = name.replace(\".csv\", \"\")\n",
    "    data[name]=file\n",
    "    \n",
    "#survey files    \n",
    "data_files = [\n",
    "\"https://raw.githubusercontent.com/TomazFilgueira/Dq_Datascientist/refs/heads/main/3%20-%20Data_Cleaning/03_Data_Cleaning_Walktrough/data/survey_all.txt\",\n",
    "\"https://raw.githubusercontent.com/TomazFilgueira/Dq_Datascientist/refs/heads/main/3%20-%20Data_Cleaning/03_Data_Cleaning_Walktrough/data/survey_d75.txt\",\n",
    "]\n",
    "\n",
    "all_survey = pd.read_csv(data_files[0],delimiter=\"\\t\",encoding=\"windows-1252\")\n",
    "d75_survey = pd.read_csv(data_files[1],delimiter=\"\\t\",encoding=\"windows-1252\")    \n",
    "    \n",
    "survey = pd.concat([all_survey,d75_survey],axis=0)\n",
    "\n",
    "survey = survey.copy()\n",
    "survey['DBN'] = survey['dbn']\n",
    "\n",
    "cols = [\"DBN\", \"rr_s\", \"rr_t\", \"rr_p\", \"N_s\", \"N_t\", \"N_p\", \"saf_p_11\", \"com_p_11\", \"eng_p_11\", \"aca_p_11\", \"saf_t_11\", \"com_t_11\", \"eng_t_11\", \"aca_t_11\", \"saf_s_11\", \"com_s_11\", \"eng_s_11\", \"aca_s_11\", \"saf_tot_11\", \"com_tot_11\", \"eng_tot_11\", \"aca_tot_11\"]\n",
    "\n",
    "#Filter survey so it only contains the columns we listed above\n",
    "survey = survey[cols]\n",
    "\n",
    "#Assign the dataframe survey to the key survey in the dictionary data.\n",
    "data['survey'] = survey\n",
    "\n",
    "#================================================================\n",
    "data['hs_directory']['DBN'] = data['hs_directory']['dbn']\n",
    "\n",
    "#===============================================================\n",
    "#if string is 2 digits long - return the string\n",
    "#if string is less than 2 digits - fills with 0 in the front\n",
    "data['class_size']['padded_csd'] = data['class_size']['CSD'].apply(lambda x: str(x).zfill(2))\n",
    "data['class_size']['DBN'] =  data['class_size']['padded_csd'] + data['class_size']['SCHOOL CODE']\n",
    "\n",
    "#convert to numeric data\n",
    "data['sat_results']['SAT Math Avg. Score'] = pd.to_numeric(data['sat_results']['SAT Math Avg. Score'],errors=\"coerce\")\n",
    "\n",
    "data['sat_results']['SAT Critical Reading Avg. Score'] = pd.to_numeric(data['sat_results']['SAT Critical Reading Avg. Score'],errors=\"coerce\")\n",
    "\n",
    "data['sat_results']['SAT Writing Avg. Score']= pd.to_numeric(data['sat_results']['SAT Writing Avg. Score'],errors=\"coerce\")\n",
    "\n",
    "#sum up all the three columns\n",
    "data['sat_results']['sat_score'] = data['sat_results']['SAT Math Avg. Score'] + data['sat_results']['SAT Critical Reading Avg. Score'] + data['sat_results']['SAT Writing Avg. Score']\n",
    "\n",
    "\n",
    "#==============================================================\n",
    "def get_lat(x):\n",
    "    #extract raw coordinates\n",
    "    y = re.findall(\"\\(.+\\)\", x)\n",
    "    \n",
    "    #split lat and lon. remove '(' for latitude\n",
    "    lat = y[0].split(',')[0].replace(\"(\",\"\")\n",
    "    return lat\n",
    "\n",
    "def find_lon(x):\n",
    "    #extract raw coordinates\n",
    "    y = re.findall(\"\\(.+\\)\", x)\n",
    "    \n",
    "    #split lat and lon. remove ')' for longitude\n",
    "    lon = y[0].split(',')[1].replace(\")\",\"\").strip()\n",
    "    return lon\n",
    "\n",
    "data['hs_directory']['lon'] = data['hs_directory']['Location 1'].apply(find_lon)\n",
    "data['hs_directory']['lat'] = data['hs_directory']['Location 1'].apply(get_lat)\n",
    "\n",
    "\n",
    "#convert coordinates to numeric\n",
    "data[\"hs_directory\"][\"lat\"] = pd.to_numeric(data[\"hs_directory\"][\"lat\"], errors=\"coerce\")\n",
    "data[\"hs_directory\"][\"lon\"] = pd.to_numeric(data[\"hs_directory\"][\"lon\"], errors=\"coerce\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9845bfef",
   "metadata": {},
   "source": [
    "# Cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43d0cc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2383/3054888080.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined['school_dist'] = combined['DBN'].apply(lambda x:x[0:2])\n"
     ]
    }
   ],
   "source": [
    "class_size = data[\"class_size\"]\n",
    "#Filter class_size so the GRADE = 09-12\n",
    "#Filter class_size so that the PROGRAM = GEN ED\n",
    "class_size = class_size[ (class_size[\"GRADE \"]==\"09-12\") & (class_size[\"PROGRAM TYPE\"]==\"GEN ED\")]\n",
    "\n",
    "#################################################\n",
    "data['demographics'] =data['demographics'][data['demographics']['schoolyear']==20112012]\n",
    "\n",
    "##################################################\n",
    "data['graduation'] = data['graduation'][(data['graduation']['Cohort']==\"2006\") & (data['graduation']['Demographic']=='Total Cohort')]\n",
    "\n",
    "######################################################\n",
    "cols = ['AP Test Takers ', 'Total Exams Taken', 'Number of Exams with scores 3 4 or 5']\n",
    "\n",
    "for c in cols:\n",
    "    data['ap_2010'][c] = pd.to_numeric(data['ap_2010']  [c],errors=\"coerce\")\n",
    "\n",
    "#####################################################\n",
    "combined = data[\"sat_results\"]\n",
    "\n",
    "#merge with ap_2010\n",
    "combined = combined.merge(data[\"ap_2010\"],how=\"left\",on=\"DBN\")\n",
    "\n",
    "#merge with \"graduation\"\n",
    "combined = combined.merge(data[\"graduation\"],how=\"left\",on=\"DBN\")\n",
    "\n",
    "#merge with class_size\n",
    "combined = combined.merge(data[\"class_size\"],how=\"inner\",on=\"DBN\")\n",
    "\n",
    "#merge with demographics\n",
    "combined = combined.merge(data[\"demographics\"],how=\"inner\",on=\"DBN\")\n",
    "\n",
    "#merge with survey\n",
    "combined = combined.merge(data[\"survey\"],how=\"inner\",on=\"DBN\")\n",
    "\n",
    "#merge with hs_directory\n",
    "combined = combined.merge(data[\"hs_directory\"],how=\"inner\",on=\"DBN\")\n",
    "\n",
    "#########################################################\n",
    "#fill NAN with mean for each column\n",
    "mean = combined.mean(numeric_only=True)\n",
    "combined.fillna(mean)\n",
    "\n",
    "#fill remaining NAN with 0\n",
    "combined = combined.infer_objects(copy=False).fillna(0)\n",
    "\n",
    "##########################################################\n",
    "#extract first two digits\n",
    "combined['school_dist'] = combined['DBN'].apply(lambda x:x[0:2])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff6ba7b",
   "metadata": {},
   "source": [
    "# 1) Introduction\n",
    "\n",
    "we began investigating possible relationships between SAT scores and demographics. In order to do this, we acquired several datasets containing information about **New York City public schools**. We **cleaned** them, then **combined** them into a single dataset named combined that we're now ready to analyze and visualize.\n",
    "\n",
    "In this lesson, we'll discover correlations, create plots, and then make maps. The first thing we'll do is find any **correlations** between `columns` and `sat_score`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a23ac3",
   "metadata": {},
   "source": [
    "# 2 - 3) Finding Correlations With the r Value\n",
    "\n",
    "We'll be using the **r value**, also called Pearson's correlation coefficient, to measure how closely two sequences of numbers are correlated.\n",
    "\n",
    "An **r value** falls between `-1` and `1`. The value determines whether two columns are:\n",
    "\n",
    "* positively correlated;\n",
    "* not correlated;\n",
    "* or negatively correlated.\n",
    "\n",
    "The closer to `1` the **r value** is, the stronger the positive correlation between the two columns. The closer to `-1` the **r value** is, the stronger the negative correlation (i.e., the more \"opposite\" the columns are). The closer to `0`, the weaker the correlation.\n",
    "\n",
    "In general, r values above `.25` or below `-.25` are enough to qualify a correlation as interesting.\n",
    "\n",
    "We can use the pandas `pandas.DataFrame.corr()` method to find correlations between columns in a dataframe. The method returns a new dataframe where the index for each column and row is the name of a column in the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b3a247",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "1. Use the `pandas.DataFrame.corr()` method on the combined dataframe to find all possible correlations. Assign the result to correlations.\n",
    "    * Ensure you use the `numeric_only=True` argument to only return the correlations between numeric columns.\n",
    "\n",
    "1. Filter correlations so that it only shows correlations for the column `sat_score`.\n",
    "1. Display all of the rows in correlations and examine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40710f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = combined.corr(numeric_only=True)['sat_score']\n",
    "print(correlations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
